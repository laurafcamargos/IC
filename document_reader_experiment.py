 # -*- coding: utf-8 -*-
"""Document Reader Experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uU-4KKD_HCSSwq1K-L708L-iYMM3mtcT

# Document Reader Experiments

In Question Answering (QA), queries are run over several documents to extract an answer to user questions, consisting of two main steps: (1) Document Retriever — retrieve the most useful documents that may contain the answer to a given question; (2) Document Reader — a machine reader carefully examines the retrieved documents and frame an answer.

In this Jupyter Notebook, we focused on the Document Reader experiments, motivated by the fact that using a good Reader (higher F1) produces a better and more concise response.

---
## Setup

Packages installation and setups.

### Run Configuration

Choose the dataset and the Document Reader algorithm.
"""

import enum

class Dataset(enum.Enum):
    '''Dataset options'''
    SQuAD = 1
    AdvQA = 2
    DuoRC = 3
    QASports = 4

class DocReader:
    '''Document Reader options'''
    BERT    = "deepset/bert-base-uncased-squad2"
    RoBERTa = "deepset/roberta-base-squad2"
    MiniLM  = "deepset/minilm-uncased-squad2"
    DistilBERT = "distilbert-base-uncased-distilled-squad"
    ELECTRA = "deepset/electra-base-squad2"
    SmallDistilBERT= "laurafcamargos/distilbert-qasports-basket-small"

class Sports:
    BASKETBALL = "basketball"
    FOOTBALL = "football"
    SOCCER = "soccer"
    ALL = ""

# run configuration
NUM_K      = 1 # always = 1
DATASET    = Dataset.QASports
DOC_READER = DocReader.BERT
SPORT = Sports.ALL

"""### Package Installation

Install Haystack and HuggingFace packages.
"""

# Check if you have a GPU running
# The code runs in CPU as well
#nvidia-smi

# %%capture
# Install the Haystack
#pip install pip --quiet
#pip install farm-haystack[colab]
#pip install farm-haystack[metrics]
# !pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]
#pip install rapidfuzz
# Install Huggingface
#pip install datasets
#pip install transformers==4.20.1 --quiet
#pip install sentence-transformers==2.2.2 --quiet
#echo "Silent installation with success!"

"""### Logging

We configure how logging messages should be displayed and which log level should be used before importing Haystack.
"""

import logging

# Setup Haystack logging format
logging.basicConfig(format="%(levelname)s - %(message)s", level=logging.WARNING)
logging.getLogger("haystack").setLevel(logging.INFO)

#pip install mmh3

"""---
## Dataset

Importing and download the respective dataset.

### Abstract Dataset
"""

import pandas as pd
from abc import ABCMeta, abstractmethod

class AbstactDataset(metaclass = ABCMeta):
    '''Abstract dataset class'''

    def __init__(self):
        self.raw_dataset = self.download()
        self.df_dataset = self._transform_df()
        print(f"## {self.name} ##")
        print(self.raw_dataset)


    def _transform_df(self):
        '''Transform dataset in a pd.DataFrame'''
        return pd.DataFrame(self.raw_dataset)

    @property
    @abstractmethod
    def name(self):
        '''Dataset name'''
        pass

    @abstractmethod
    def download(self):
        '''Download the dataset'''
        pass

    @abstractmethod
    def get_documents(self):
        '''Get the unique documents to store in the Document Store'''
        pass


    @abstractmethod
    def get_validation(self):
        '''Get the validation set'''
        pass

"""### SQuaD Dataset

https://huggingface.co/datasets/squad
"""

import mmh3
from datasets import load_dataset
from haystack.schema import Label, Document, Answer
from haystack.schema import EvaluationResult, MultiLabel

class SQuadDataset(AbstactDataset):
    '''SQuaD Dataset'''
    name = "SQuaD Dataset"
    _columns = {
        "title": "title",
        "document": "context",
        "question": "question",
    }
    _metadata = {
        "dataset_id": "id"
    }

    def download(self):
        dataset = load_dataset("squad", split="validation")
        return dataset

    def get_documents(self):
        # Remove duplicated contents
        cc = self._columns
        dataset_name = f"{self.name}"
        df = self.df_dataset
        df = df.drop_duplicates(subset=[cc["title"], cc["document"]], keep="first")

        # Create Haystack Document objects
        list_docs = []
        skipped_count = 0  # contador de linhas puladas
        for _, row in df.iterrows():
            document_value = row[cc["document"]]
            if document_value is not None:
                document_value_bytes = str(document_value).encode('utf-8')  # Converte para bytes
                document_id = mmh3.hash128(document_value_bytes, signed=False)
                doc_metadata = {k: row[v] for k, v in self._metadata.items()}
                doc_metadata["title"] = row[cc["title"]]
                doc_metadata["dataset_name"] = dataset_name
                doc = Document(
                    id=document_id,
                    content_type="text",
                    content=document_value,
                    meta=doc_metadata
                )
                list_docs.append(doc)
            else:
                # Imprimir toda a linha do DataFrame correspondente ao documento
                print(f"Warning: 'document' is None for this row. Skipping document:")
                print(row)
                skipped_count += 1

        print(f"Total rows skipped due to 'document' being None: {skipped_count}")

        return list_docs

    def _get_answers(self, data):
        # Get question answer
        return data["answers"]["text"]

    def get_validation(self):
        # Get dataset info
        cc = self._columns
        df = self.df_dataset
        _self = self

        # Create Haystack labels
        eval_labels = []
        for _, row in df.iterrows():
            document_value = row[cc["document"]]
            if document_value is not None:
                document_id = mmh3.hash128(document_value, signed=False)
                doc_label = MultiLabel(labels=[
                    Label(
                        query=row[cc["question"]],
                        answer=Answer(answer=answer, type="extractive"),
                        document=Document(
                            id=document_id,
                            content_type="text",
                            content=document_value,
                        ),
                        is_correct_answer=True,
                        is_correct_document=True,
                        origin="gold-label",
                    )
                    for answer in _self._get_answers(row)
                ])
                eval_labels.append(doc_label)
            else:
                # Tratar caso em que o valor é None
                print("Warning: 'document' is None for this row in get_validation. Skipping...")

        return eval_labels

"""### AdversarialQA Dataset

https://huggingface.co/datasets/adversarial_qa
"""

class AdversarialQADataset(SQuadDataset):
    '''AdversarialQA Dataset'''
    name = "AdversarialQA Dataset"

    def download(self):
        dataset = load_dataset("adversarial_qa", "adversarialQA", split="validation")
        return dataset

"""### DuoRC Dataset

https://huggingface.co/datasets/duorc
"""

class DuoRCDataset(SQuadDataset):
    '''DuoRC  Dataset'''
    name = "DuoRC Dataset"
    _columns = {
        "title": "title",
        "document": "plot",
        "question": "question",
    }
    _metadata = {
        "dataset_id": "question_id"
    }

    def download(self):
        dataset = load_dataset("duorc", "SelfRC", split="validation")
        return dataset

    def _transform_df(self):
        '''Transform dataset in a pd.DataFrame'''
        df = pd.DataFrame(self.raw_dataset)
        # Get questions with answer
        return df[~df["no_answer"]]

    def _get_answers(self, data):
        # Get question answer
        print(data)
        return data["answers"]

"""###QASports Dataset
https://huggingface.co/datasets/PedroCJardim/QASports
"""

import ast

class QASportsDataset(SQuadDataset):
    '''QASports Dataset'''

    name = "QASports Dataset"

    _columns = {
        "title": "context_title",
        "document": "context",
        "question": "question",
    }
    _metadata = {
        "dataset_id": "id_qa"
    }

    def __init__(self, sport=None):
        self.sport = sport
        super().__init__()  # chama o construtor da classe base

    def download(self):
        if self.sport is not None:
            dataset = load_dataset("PedroCJardim/QASports", self.sport, split="validation")
            return dataset
        else:
            dataset = load_dataset("PedroCJardim/QASports", split="validation")
            return dataset
    def _get_answers(self, data):
        # Converte a string que representa um dicionário em um dicionário Python
        answer_dict = ast.literal_eval(data["answer"])

        # Retorna uma lista com o texto da resposta
        return [answer_dict["text"]]

"""### Download the dataset

Get the dataset and store the documents in the Document Store.
"""

def dataset_switch(choice):
    '''Get dataset class'''

    if choice == Dataset.SQuAD:
        return SQuadDataset()
    elif choice == Dataset.AdvQA:
        return AdversarialQADataset()
    elif choice == Dataset.DuoRC:
        return DuoRCDataset()
    elif choice == Dataset.QASports:
      return QASportsDataset(SPORT)
    else:
      return "Invalid dataset"

# Get the dataset
dataset = dataset_switch(DATASET)
docs = dataset.get_documents()

"""---
## Document Reader

In this experiment, we explored three Transformer based models for extractive Question Answering using the [FARM framework](https://github.com/deepset-ai/FARM).
* [BERT paper](https://arxiv.org/abs/1810.04805), [implementation](https://huggingface.co/deepset/bert-base-uncased-squad2)
* [RoBERTa paper](https://arxiv.org/abs/1907.11692), [implementation](https://huggingface.co/deepset/roberta-base-squad2)
* [MiniLM paper](https://arxiv.org/abs/2002.10957), [implementation](https://huggingface.co/deepset/minilm-uncased-squad2)

"""

from haystack.nodes import FARMReader

# Get the reader
reader = FARMReader(DOC_READER, use_gpu=True)

from haystack import Pipeline

# Build the pipeline
pipe = Pipeline()
pipe.add_node(component=reader, name='Reader', inputs=['Query'])

# Testing the pipeline
from haystack.utils import print_answers

# Querying documents
question = "Who was the winner of the first round match Algeciras v Novelda?"
prediction = pipe.run(query=question, documents=docs, params={"Reader": {"top_k": 3}})

# Print answer
print_answers(prediction)

"""---
## Evaluation

About the metrics, you can read the [evaluation](https://docs.haystack.deepset.ai/docs/evaluation) web page.
"""
from pprint import pprint
import time
# Início da medição de tempo
start_time = time.time()
# Commented out IPython magic to ensure Python compatibility.
# 
# # For testing purposes, running on the first 100 labels
# # For real evaluation, you must remove the [0:100]
eval_labels = dataset.get_validation()
eval_docs = [[label.document for label in multi_label.labels] for multi_label in eval_labels] 
eval_result = pipe.eval(labels=eval_labels, documents=eval_docs, params={"Reader": {"top_k": NUM_K}})

end_time = time.time()

# Exibindo o tempo de execução
print(f"Tempo de execução: {end_time - start_time} segundos")

pprint(eval_labels[0])

from pprint import pprint

# Get and print the metrics
metrics = eval_result.calculate_metrics()
pprint(metrics)

# Print a detailed report
pipe.print_eval_report(eval_result)
